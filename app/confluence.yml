---
settings:
  artifacts-dir: results/confluence/%Y-%m-%d_%H-%M-%S
  aggregator: consolidator
  verbose: false
  check-updates: false    # disable bzt check for updates
  env:
    application_hostname: c7perf.smartics.mobi   # Confluence DC hostname without protocol and port e.g. test-confluence.atlassian.com or localhost
    application_protocol: https      # http or https
    application_port: 443              # 80, 443, 8080, 1990, etc
    secure: True                     # Set False to allow insecure connections, e.g. when using self-signed SSL certificate
    application_postfix:            # e.g. /confluence in case of url like http://localhost:1990/confluence
    admin_login: anton.kronseder 
    admin_password: weltkome
    load_executor: locust           # jmeter and locust are supported. jmeter by default.

    concurrency: 4                 # number of concurrent virtual users for jmeter or locust scenario
    test_duration: 3m
    ramp-up: 10s                     # time to spin all concurrent users
    total_actions_per_hour: 2000

    # DATACENTER in k8 AWS
    #application_hostname: ad1aff17e0f7641b1861fa54a67cc4b1-1777738829.us-east-2.elb.amazonaws.com # Confluence DC hostname without protocol and port e.g. test-confluence.atlassian.com or localhost
    #application_protocol: http      # http or https
    #application_port: 80             # 80, 443, 8080, 1990, etc
    #secure: False                     # Set False to allow insecure connections, e.g. when using self-signed SSL certificate
    #application_postfix:            /confluence # e.g. /confluence in case of url like http://localhost:1990/confluence
    #admin_login: admin
    #admin_password: admin
    #load_executor: locust           # jmeter and locust are supported. jmeter by default.
    #concurrency: 200                # number of concurrent virtual users for jmeter or locust scenario
    #test_duration: 45m
    #ramp-up: 5m                       # time to spin all concurrent users
    #total_actions_per_hour: 20000

    # smartics developer tests
    #    concurrency: 4                # number of concurrent virtual users for jmeter or locust scenario
    #    test_duration: 1m
    #    ramp-up: 5s                     # time to spin all concurrent users
    #total_actions_per_hour: 2000

    WEBDRIVER_VISIBLE: False
    JMETER_VERSION: 5.4.3
    LANGUAGE: en_US.utf8
    allow_analytics: Yes              # Allow sending basic run analytics to Atlassian. These analytics help us to understand how the tool is being used and help us to continue to invest in this tooling. For more details please see our README.
    # Action percentage for JMeter and Locust load executors

    extended_metrics: False
    view_page: 33
    view_dashboard: 9
    view_blog: 13
    search_cql: 10
    create_blog: 4
    create_and_edit_page: 8
    comment_page: 7
    view_attachment: 5
    upload_attachment: 6
    like_page: 3
    upload_emoticon: 2      # For Confluence 8.4.x+

    #    view_page: 0
    #    view_dashboard: 0
    #    view_blog: 0
    #    search_cql: 0
    #    create_blog: 0
    #    create_and_edit_page: 0
    #    comment_page: 0
    #    view_attachment: 0
    #    upload_attachment: 0
    #    like_page: 0
    #    upload_emoticon: 0      # For Confluence 8.4.x+
    standalone_extension: 10 # By default disabled

    # Custom dataset section.
    #    custom_dataset_query:  # Write CQL query to add CQL output to the app/datasets/confluence/custom_pages.csv, e.g. "title ~ 'AppPage*'"
    # custom_dataset_query:  # Write CQL query to add CQL output to the app/datasets/confluence/custom_pages.csv, e.g. "title ~ 'AppPage*'"
    # Write CQL query to add CQL output to the app/datasets/confluence/custom_pages.csv, e.g. "title ~ 'AppPage*'"
    custom_dataset_query:  "space='PROJECTDOCTEST' AND (title = 'Test Case Display Table' OR title =  'Test Case Transclude from Documents')" 
    # custom_dataset_query:   # Write CQL query to add CQL output to the app/datasets/confluence/custom_pages.csv, e.g. "title ~ 'AppPage*'"
    # custom_dataset_query: title ~ 'smarticsUserScriptPage'          # Write CQL query to add CQL output to the app/datasets/confluence/custom_pages.csv, e.g. "title ~ 'AppPage*'"
services:
  - module: shellexec
    prepare:
      - python util/pre_run/environment_checker.py
      - python util/pre_run/check_for_updates.py
      - python util/data_preparation/confluence_prepare_data.py
    shutdown:
      - python util/post_run/jmeter_post_check.py
      - python util/jtl_convertor/jtls-to-csv.py kpi.jtl selenium.jtl
    post-process:
      - python util/analytics/analytics.py confluence
      - python util/post_run/cleanup_results_dir.py
  - module: pip-install
    packages:
      - selenium==4.9.0
execution:
  - scenario: ${load_executor}
    executor: ${load_executor}
    concurrency: ${concurrency}
    hold-for: ${test_duration}
    ramp-up: ${ramp-up}
  - scenario: selenium
    executor: selenium
    runner: pytest
    hold-for: ${test_duration}
scenarios:
  selenium:
    script: selenium_ui/confluence_ui.py
  locust:
    script: locustio/confluence/locustfile.py
  jmeter:
    script: jmeter/confluence.jmx
    properties:
      application_hostname: ${application_hostname}
      application_protocol: ${application_protocol}
      application_port: ${application_port}
      application_postfix: ${application_postfix}
      # Workload model
      total_actions_per_hr: ${total_actions_per_hour}
      perc_view_page: ${view_page}
      perc_view_dashboard: ${view_dashboard}
      perc_view_blog: ${view_blog}
      perc_search_cql: ${search_cql}
      perc_create_blog: ${create_blog}
      perc_create_and_edit_page: ${create_and_edit_page}
      perc_comment_page: ${comment_page}
      perc_view_attachment: ${view_attachment}
      perc_upload_attachment: ${upload_attachment}
      perc_like_page: ${like_page}
      perc_upload_emoticon: ${upload_emoticon}
      perc_standalone_extension: ${standalone_extension}
modules:
  consolidator:
    rtimes-len: 0 # CONFSRVDEV-7631 reduce sampling
    percentiles: [] # CONFSRVDEV-7631 disable all percentiles due to Taurus's excessive memory usage
  jmeter:
    version: ${JMETER_VERSION}
    detect-plugins: true
    memory-xmx: 8G  # allow JMeter to use up to 8G of memory
    plugins:
      - bzm-parallel=0.4
      - bzm-random-csv=0.6
      - jpgc-casutg=2.5
      - jpgc-dummy=0.2
      - jpgc-ffw=2.0
      - jpgc-fifo=0.2
      - jpgc-functions=2.1
      - jpgc-json=2.6
      - jpgc-perfmon=2.1
      - jpgc-prmctl=0.4
      - jpgc-tst=2.4
      - jpgc-wsc=0.3
      - tilln-sshmon=1.0
      - jpgc-synthesis=2.2
    system-properties:
      server.rmi.ssl.disable: true
      java.rmi.server.hostname: localhost
      httpsampler.ignore_failed_embedded_resources: "true"
  selenium:
    chromedriver:
#      version: "101.0.4951.41" # Supports Chrome version 101. You can refer to http://chromedriver.chromium.org/downloads
#      version: "90.0.4430.24" # Supports Chrome version 97. You can refer to http://chromedriver.chromium.org/downloads
#      version: "102.0.5005.61" # Supports Chrome version 102. You can refer to http://chromedriver.chromium.org/downloads
#      version: "103.0.5060.134" # Supports Chrome version 103. You can refer to http://chromedriver.chromium.org/downloads
      version: "114.0.5735.90" # Supports Chrome version 103. You can refer to http://chromedriver.chromium.org/downloads
  blazemeter:
    # token: e41bf0cde4ff5c5d75ba5e5ee9aa200e85bfe870a51262efdf4a4e6dcb084f3a677dcb25 
    # address: https://a.blazemeter.com  # reporting service address
    # data-address: https://data.blazemeter.com  # data service address
    browser-open: none  # auto-open the report in browser, 
    # can be "start", "end", "both", "none"
    # send-interval: 30s   # send data each n-th second
    # timeout: 5s  # connect and request timeout for BlazeMeter API
    # artifact-upload-size-limit: 5  # limit max size of file (in megabytes) 
    # that goes into zip for artifact upload, 10 by default
    # following instructions will have effect when no per-reporter settings
    report-name:  ask # if you will use value 'ask', it will ask it from command line
    #  modules.blazemeter.report-name="example-1"
    test: regression-example 
    project: projectdoc 
reporting:
- data-source: sample-labels
  module: junit-xml
- module: passfail
  criteria:
  - failures>25% over 300s, stop as failed
